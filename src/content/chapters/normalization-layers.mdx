# Normalization Layers

Layers like BatchNorm and LayerNorm help stabilize training by normalizing the activations.
